{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a hyperdictionary\n",
    "\n",
    "I was considering that the basic way to solve the letter prediction problem given no constraints, would be to just have a dictionary of words, and then be able to reference that dictionary. I am attempting to store a dictionary into a hypervector and create a hyperdictionary.\n",
    "\n",
    "The hypervectors are very similar to hashes, and so each word or subword has no relationship to the hash. So in order to store a dictionary in the hyper vector, you need to store the word and all of the substrings. \n",
    "\n",
    "Essentially, I am encoding an algorithm in the hypervector that does a tree search through a dictionary. I want to start typing in letters and then have the hyperdictionary list the possible next letters given the words that are stored. This means I want to store not only everyword, but the entire tree of substrings that make up the word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height has been deprecated.\n",
      "\n",
      "2016-02-24 09:47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random_idx\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from pylab import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the hyper dictionary\n",
    "\n",
    "So, I have gone to the internet and just found a text file that contains a list of common english words. My goal is to put this dictionary into a hyper vector and then see if I can use a standard word-based algorithm to predict the next letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdict = open(\"2of12id.txt\")\n",
    "word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in fdict:\n",
    "    words = line.split()\n",
    "    \n",
    "    # take out the noun/verb/adjective\n",
    "    words.pop(1)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.find('{') > 0:\n",
    "            continue\n",
    "            \n",
    "        w = word.strip('()~-|{}!@/')\n",
    "        \n",
    "        if len(w) == 0:\n",
    "            continue\n",
    "                \n",
    "        word_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100060\n"
     ]
    }
   ],
   "source": [
    "print len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a dictionary of over 100,000 words now. I am going to go through each word, substring by substring, and add each of the substrings to the hypervector. This means that there will be far more than 100k elements that need to be stored in the hypervector, because I am essentially trying to store the entire tree. Since there are so many words, I am going to start using an even larger hyper-vector. There will be issues with how much information we can store in the hypervectors, and there is already some literature on this. \n",
    "\n",
    "I really want the hyper vector to just work like a word dictionary. I am only going to add a substring if it is not already present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=1000000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "print letter_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperdictionary = np.zeros(N)\n",
    "count = 0\n",
    "vals = []\n",
    "subwords = []\n",
    "skip = 20\n",
    "\n",
    "for word in word_list[0::skip]:\n",
    "#for word in ['accelerate','aardvark', 'accordion', 'accordionists',  'apple', 'betazoid', 'betakeratine']:\n",
    "#for word in ['a', 'b', 'c', 'd','e', 'f']:\n",
    "    #print \"\"\n",
    "    print word,\n",
    "    subword = ''\n",
    "    subvec = np.ones(N)\n",
    "    for i,letter in enumerate(word):\n",
    "        letter_idx = random_idx.alphabet.find(letter)\n",
    "        subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "        subword += letter\n",
    "        \n",
    "        # check to see if the subvec is already present in the hyperdictionary\n",
    "        val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "        # If the substring is not present, then val should be near 0\n",
    "        if val < 0.4:\n",
    "            # then add the substring\n",
    "            hyperdictionary += subvec\n",
    "            count += 1\n",
    "            #print subword, \n",
    "    \n",
    "    letter_idx = random_idx.alphabet.find(' ')\n",
    "    subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "    # check to see if the subvec is already present in the hyperdictionary\n",
    "    val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "    # If the substring is not present, then val should be near 0\n",
    "    if val < 0.4:\n",
    "        # then add the subaQstring\n",
    "        hyperdictionary += subvec\n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_idx.alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/hyperdictionary_external-s20-d1M-160223.npz', hyperdictionary=hyperdictionary, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdict = open(\"raw_texts/texts_english/alice_in_wonderland.txt\")\n",
    "text = fdict.read().lower()\n",
    "\n",
    "punct = string.punctuation + string.digits\n",
    "\n",
    "for i in punct:\n",
    "    if i == '-':\n",
    "        text = text.replace(i, ' ')\n",
    "    else:\n",
    "        text = text.replace(i, '')\n",
    "    \n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('\\r','')\n",
    "text = text.replace('\\t','')\n",
    "short_text = text[504:137330]\n",
    "\n",
    "word_list = set(text.split()[1:]);\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_text = text[504:137330]\n",
    "word_list = set(short_text.split()[1:]);\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N=1000000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "\n",
    "hyperdictionary = np.zeros(N)\n",
    "count = 0\n",
    "vals = []\n",
    "subwords = []\n",
    "skip = 20\n",
    "\n",
    "for word in word_list:\n",
    "#for word in ['accelerate','aardvark', 'accordion', 'accordionists',  'apple', 'betazoid', 'betakeratine']:\n",
    "#for word in ['a', 'b', 'c', 'd','e', 'f']:\n",
    "    #print \"\"\n",
    "    print word,\n",
    "    subword = ''\n",
    "    subvec = np.ones(N)\n",
    "    for i,letter in enumerate(word):\n",
    "        letter_idx = random_idx.alphabet.find(letter)\n",
    "        subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "        subword += letter\n",
    "        \n",
    "        # check to see if the subvec is already present in the hyperdictionary\n",
    "        val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "        # If the substring is not present, then val should be near 0\n",
    "        if val < 0.4:\n",
    "            # then add the substring\n",
    "            hyperdictionary += subvec\n",
    "            count += 1\n",
    "            #print subword, \n",
    "    \n",
    "    letter_idx = random_idx.alphabet.find(' ')\n",
    "    subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "    # check to see if the subvec is already present in the hyperdictionary\n",
    "    val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "    # If the substring is not present, then val should be near 0\n",
    "    if val < 0.4:\n",
    "        # then add the substring\n",
    "        hyperdictionary += subvec\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/hyperdictionary_alice-short-d1M-160223.npz', hyperdictionary=hyperdictionary, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## N-gram statistics\n",
    "\n",
    "Now, going to make a hypervector that keeps stats on the 2-grams of letters in the text (including spaces). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_text = text[504:137330]\n",
    "print short_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate text vector based on each pair of characters\n",
    "\n",
    "N=20000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "\n",
    "alice_text_vector2 = random_idx.generate_text_vector(N, letter_vectors, 2, short_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_text_vector2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/alice-2gram-space-d20K-160223.npz', hyperdictionary=alice_text_vector2, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "N=20000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "\n",
    "alice_text_vector3 = random_idx.generate_text_vector(N, letter_vectors, 3, short_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/alice-3gram-space-d20K-160223.npz', hyperdictionary=alice_text_vector3, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
