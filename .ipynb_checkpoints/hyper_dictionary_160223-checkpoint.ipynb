{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a hyperdictionary\n",
    "\n",
    "I was considering that the basic way to solve the letter prediction problem given no constraints, would be to just have a dictionary of words, and then be able to reference that dictionary. I am attempting to store a dictionary into a hypervector and create a hyperdictionary.\n",
    "\n",
    "The hypervectors are very similar to hashes, and so each word or subword has no relationship to the hash. So in order to store a dictionary in the hyper vector, you need to store the word and all of the substrings. \n",
    "\n",
    "Essentially, I am encoding an algorithm in the hypervector that does a tree search through a dictionary. I want to start typing in letters and then have the hyperdictionary list the possible next letters given the words that are stored. This means I want to store not only everyword, but the entire tree of substrings that make up the word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import random_idx\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "import string\n",
    "from pylab import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the hyper dictionary\n",
    "\n",
    "So, I have gone to the internet and just found a text file that contains a list of common english words. My goal is to put this dictionary into a hyper vector and then see if I can use a standard word-based algorithm to predict the next letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdict = open(\"2of12id.txt\")\n",
    "word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in fdict:\n",
    "    words = line.split()\n",
    "    \n",
    "    # take out the noun/verb/adjective\n",
    "    words.pop(1)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.find('{') > 0:\n",
    "            continue\n",
    "            \n",
    "        w = word.strip('()~-|{}!@/')\n",
    "        \n",
    "        if len(w) == 0:\n",
    "            continue\n",
    "                \n",
    "        word_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100060\n"
     ]
    }
   ],
   "source": [
    "print len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a dictionary of over 100,000 words now. I am going to go through each word, substring by substring, and add each of the substrings to the hypervector. This means that there will be far more than 100k elements that need to be stored in the hypervector, because I am essentially trying to store the entire tree. Since there are so many words, I am going to start using an even larger hyper-vector. There will be issues with how much information we can store in the hypervectors, and there is already some literature on this. \n",
    "\n",
    "I really want the hyper vector to just work like a word dictionary. I am only going to add a substring if it is not already present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  1 -1 ...,  1 -1  1]\n",
      " [-1 -1  1 ..., -1 -1  1]\n",
      " [-1 -1 -1 ..., -1 -1  1]\n",
      " ..., \n",
      " [-1 -1  1 ..., -1  1 -1]\n",
      " [ 1 -1  1 ..., -1  1 -1]\n",
      " [ 1 -1  1 ...,  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "N=1000000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "print letter_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperdictionary = np.zeros(N)\n",
    "count = 0\n",
    "vals = []\n",
    "subwords = []\n",
    "skip = 20\n",
    "\n",
    "for word in word_list[0::skip]:\n",
    "#for word in ['accelerate','aardvark', 'accordion', 'accordionists',  'apple', 'betazoid', 'betakeratine']:\n",
    "#for word in ['a', 'b', 'c', 'd','e', 'f']:\n",
    "    #print \"\"\n",
    "    print word,\n",
    "    subword = ''\n",
    "    subvec = np.ones(N)\n",
    "    for i,letter in enumerate(word):\n",
    "        letter_idx = random_idx.alphabet.find(letter)\n",
    "        subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "        subword += letter\n",
    "        \n",
    "        # check to see if the subvec is already present in the hyperdictionary\n",
    "        val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "        # If the substring is not present, then val should be near 0\n",
    "        if val < 0.4:\n",
    "            # then add the substring\n",
    "            hyperdictionary += subvec\n",
    "            count += 1\n",
    "            #print subword, \n",
    "    \n",
    "    letter_idx = random_idx.alphabet.find(' ')\n",
    "    subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "    # check to see if the subvec is already present in the hyperdictionary\n",
    "    val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "    # If the substring is not present, then val should be near 0\n",
    "    if val < 0.4:\n",
    "        # then add the subaQstring\n",
    "        hyperdictionary += subvec\n",
    "        count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_idx.alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/hyperdictionary_external-s20-d1M-160223.npz', hyperdictionary=hyperdictionary, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3060"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdict = open(\"raw_texts/texts_english/alice_in_wonderland.txt\")\n",
    "text = fdict.read().lower()\n",
    "\n",
    "punct = string.punctuation + string.digits\n",
    "\n",
    "for i in punct:\n",
    "    if i == '-':\n",
    "        text = text.replace(i, ' ')\n",
    "    else:\n",
    "        text = text.replace(i, '')\n",
    "    \n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('\\r','')\n",
    "text = text.replace('\\t','')\n",
    "\n",
    "    \n",
    "word_list = set(text.split()[1:]);\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperdictionary = np.zeros(N)\n",
    "count = 0\n",
    "vals = []\n",
    "subwords = []\n",
    "skip = 20\n",
    "\n",
    "for word in word_list:\n",
    "#for word in ['accelerate','aardvark', 'accordion', 'accordionists',  'apple', 'betazoid', 'betakeratine']:\n",
    "#for word in ['a', 'b', 'c', 'd','e', 'f']:\n",
    "    #print \"\"\n",
    "    print word,\n",
    "    subword = ''\n",
    "    subvec = np.ones(N)\n",
    "    for i,letter in enumerate(word):\n",
    "        letter_idx = random_idx.alphabet.find(letter)\n",
    "        subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "        subword += letter\n",
    "        \n",
    "        # check to see if the subvec is already present in the hyperdictionary\n",
    "        val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "        # If the substring is not present, then val should be near 0\n",
    "        if val < 0.4:\n",
    "            # then add the substring\n",
    "            hyperdictionary += subvec\n",
    "            count += 1\n",
    "            #print subword, \n",
    "    \n",
    "    letter_idx = random_idx.alphabet.find(' ')\n",
    "    subvec = np.roll(subvec, 1) * letter_vectors[letter_idx,:]\n",
    "    # check to see if the subvec is already present in the hyperdictionary\n",
    "    val = np.dot(subvec.T, hyperdictionary) / N\n",
    "        \n",
    "    # If the substring is not present, then val should be near 0\n",
    "    if val < 0.4:\n",
    "        # then add the substring\n",
    "        hyperdictionary += subvec\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/hyperdictionary_alice-d1M-160223.npz', hyperdictionary=hyperdictionary, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## N-gram statistics\n",
    "\n",
    "Now, going to make a hypervector that keeps stats on the 2-grams of letters in the text (including spaces). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'random_idx' from 'random_idx.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(random_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n"
     ]
    }
   ],
   "source": [
    "# generate text vector based on each pair of characters\n",
    "text_name=\"preprocessed_texts/AliceInWonderland.txt\"\n",
    "\n",
    "N=10000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "\n",
    "alice_text_vector2 = random_idx.generate_RI_text_fast(N, letter_vectors, 2, 0, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_text_vector2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/alice-2gram-space-d10K-160223.npz', hyperdictionary=alice_text_vector2, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 43000 44000 45000 46000 47000 48000 49000 50000 51000 52000 53000 54000 55000 56000 57000 58000 59000 60000 61000 62000 63000 64000 65000 66000 67000 68000 69000 70000 71000 72000 73000 74000 75000 76000 77000 78000 79000 80000 81000 82000 83000 84000 85000 86000 87000 88000 89000 90000 91000 92000 93000 94000 95000 96000 97000 98000 99000 100000 101000 102000 103000 104000 105000 106000 107000 108000 109000 110000 111000 112000 113000 114000 115000 116000 117000 118000 119000 120000 121000 122000 123000 124000 125000 126000 127000 128000 129000 130000 131000 132000 133000 134000 135000 136000 137000 138000 139000 140000 141000 142000 143000 144000 145000 146000 147000 148000 149000 150000 151000 152000 153000 154000 155000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N=50000\n",
    "letter_vectors = 2 * (np.random.randn(len(random_idx.alphabet), N) > 0) - 1\n",
    "\n",
    "alice_text_vector3 = random_idx.generate_RI_text_fast(N, letter_vectors, 3, 0, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('data/alice-3gram-space-d50K-160223.npz', hyperdictionary=alice_text_vector3, letter_vectors=letter_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 50000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
