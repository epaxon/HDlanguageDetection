Hi Quinn and Anna,

Here is an exercise we can start with, to learn about
language vectors based on n-grams.  So far we have
talked about language vectors built from trigrams.
However, Aditya and Johan made some also from bigrams,
tetragrams and pentagrams and found out that tetragrams
make slightly better vectors than trigrams, and
pentagrams make slightly worse.  (Can you think of why
pentagrams would be worse than tetragrams?)

We can also sum these different language vectors into a
single vector and use it for finding words.  I think we
should do that next.  Do you think you can change the
program so that it produces a language vector that is
the sum of six language vectors, one that is based on
single letters, another that is based on bigrams, third
based on trigrams, fourth on tetragrams, fifth on
pentagrams, and the sixth based on hexagrams.  You
can use Alice in Wonderland for data:

  http://www.gutenberg.org/ebooks/11

Remove all non-letters from the text and make all
letters the same case (upper of lower).  That should be
about 100,000 letters.

How this combination vector can be used to find the
number of different letters, bigrams, trigrams, etc. in
the text will be the next exercise.  You can start
thinking about it.

See you tomorrow,
Pentti

Hi Anna, hello Quinn,

Was it midterms last week?  I hope you are please with
how you did.

Your ideas, questions and speculations about n-grams
are right on and we could use language identification
to test them further.  For example, there are 14,348,907
possible pentagrams of 16 letters and the space, and
although most of them never occur, the number of ones
that do is too large to be covered well with a million
bytes of text, and so the language vector would be
specific to the data set.

We may return to language identification later but
right now I have a different task in mind.  We will
start with text that has no spaces and try to find
where spaces should be inserted to make it look more
like English.  I hope to do this first with 10,000-D
language vectors but without additional (sparse
distributed) memory.  In last week's email I described
the language vectors, and they are straightforward to
compute as you say.  The algorithm is simple.  The
interesting thing is in seeing if and how it can be
made to work for finding words.

Besides the tree of us, several others here are
interested in this problem and I am thinking of
including them in the discussion.  Aditya Joshi
(with Johan Halseth) wrote the original program and
the paper and he has shared his Python code with us.
He graduated in the spring, is working in industry this
year but had plans for graduate study.  Abbas Rahimi is
an EE postdoc and made Aditya's program into an on-line
demo.  Mika Laiho is an engineering professor visiting
with us from Finland.  He wrote a program akin to
Redlich's that combines individual letters into word
fragments and words, which is the opposite of what we'd
do.  You should feel free to send email just to me, but
I'll include them in the discussion with a CC.  My next
email will be to fly this plan by them.

Hi Anna and Quinn,

I've been thinking of exercises to help us get familiar
with language vectors.  We start with 26 independent
random 10,000-D letter vectors with equally probable 1s
and -1s; make it 27 to include the space.  I think
Aditya's program makes them with exactly 5,000 1s and
-1.  It is also fine if they are based on 10,000 tosses
of a coin, because once such vectors are multiplied
together (pointwise), we no longer have exactly 5,000
anyway.

If A, B, C, ... denote the vectors for single letters
"a" "b" "c" ..., then the trigram "abc" is encoded with
ABC = ssA * sB * C, where s is a circular shift by one
position, and * is pointwise multiplication (see the
poster).  Notice that n-grams can be used to encode
(n+1)-grams.  For example, ABCD = s(ABC) * D.

Using the Alice text, make separate language vectors
for the different n-gram sizes (n = 1,2,3, ...,8).  We
will add them together later into a single language
vector.  That may involve weighting them unequally--to
be determined.

Take the language vector representing single letters of
Alice and compute its dot product with the 26 different
letter vectors.  Can you see a relation between the dot
products and the letter counts in Alice?  Alice has
about 300 instances of "q".  I'd expect a dot product
with the Q-vector to be around 3 million.

Then take the language vector representing the bigrams
of Alice and compute its dot product with QU.  What do
you get?  And what is this language vector's dot
product with Q?

Next, add the two language vectors into a single vector
that represents both individual letters and bigrams.
Compute its dot product with Q and with QU.  What do
you get?

One more set of tests: Take the language vector for
bigrams and (pointwise) multiply it with sQ (Q shifted
once).  Compare the resulting vector (with dot product
or cosine) to the letter vectors A, B, C, ....  Which
letter wins?  Do the same using the language vector for
individual letters, and once more using a language
vector that is the sum of the above two.

At this point, you will be totally confused or
absolutely delighted.  Please let me know.

Good luck, best wishes!