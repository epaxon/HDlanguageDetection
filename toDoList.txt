Thanks for "results.txt."  I also received your
invitation to Bitbucket but I don't know how to use it.
I probably tried with Aditya and Johan but got all
tangled up.  That may have been Gidhub.

Out of curiosity, do we have the same Alice text?  I
got 167,518 bytes of "pg11.txt" from Project Gutenberg
and it gave me 122,989 bytes of letters only.  What
size files you have?

I am happy to see your results.  The first set looks
correct to me, where you take the dot product of letter
vectors with the single-letter language vector.  Those
dot products should be approximately 10,000 times the
letter frequencies in the text.  Perhaps you already
know why.

The line

  dot product of up2_lang_vec and q is 3472592

of "results.txt" looks right to me--the number is close
to 3485012--and

  dot product of up2_lang_vec and qu is 1434872

may also be right, but

  dot product of bigrams vector and qu is -7924

seems wrong.  I'd expect the number to be well over
a million.

I am not sure what is happening in the last two blocks.
The numbers get to the millions, but some are negative
millions.  Wrong vectors may have been multiplied.

Here is some of the math.  The dot product of a vector
of 10,000 1s and -1s with itself is 10,000, and every
time a particular letter or bigram or trigram
... appears in the text, it adds 10,000 to its dot
product with the language vector.  Everything else that
goes into the language vector acts as random noise.
When 1s and -1 are equally likely, the noise nearly
cancels out and the result will be approximately 10K
times the true frequency.

Once you see how that works, it is easy to see what
the dot product between a language vector and any
particular letter or n-gram vector should be.  How ever
many times the letter or the n-gram has been added into
the language vector, just multiply it by 10K.

So try this: make a language vector of bigrams only.
If you test it with (calculate its dot produce with)
the letter vectors, you should get numbers like those
in the second block of "results.txt" that starts with

  dot product of bigrams_sQ and a is -31528

Some are positive, some negative, and none is in the
hundreds of thousands.  All that is "noise" in
signal-processing terms.  But if you test it with the
vector for QU (which equals sQ * U) or with any other
bigram vector, you should get that bigram's frequency
times 10K.  That's referred to as "signal."

Now it gets more subtle and interesting.  Take the
language vector for bigrams and MULTIPLY it
(coordinate-wise) with Q that has been rotated once
(i.e., multiply it with sQ) and then test the result
with the letter vectors.  The dot product should be
high for U only.  Can you figure out the reason why,
do you see what's happening?

After you have gotten this far, make a language vector
that combines individual letters, bigrams and trigrams:
just add those three language vectors into a single
vector (by normal vector addition.).  Then test it for
single letters and bigrams as above.  Also, multiply it
with sQ and test the result as above.  The dot products
should be close to what you got before.

Please let me know how it goes.  And have fun!

Pentti

___________ 8< _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
On Fri, 20 Nov 2015 13:00:12 -0800
QUINN THUY TRAN <ssquinntran@berkeley.edu> writes:

  These are the results.

  [2:text/plain Show Save:exercise_results.txt (5kB)]


_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ >8 ___________
On Tue, 10 Nov 2015 16:04:19 -0800
Pentti Kanerva <pkanerva@csli.stanford.edu> writes:

  Hi Anna and Quinn,

  I've been thinking of exercises to help us get familiar
  with language vectors.  We start with 26 independent
  random 10,000-D letter vectors with equally probable 1s
  and -1s; make it 27 to include the space.  I think
  Aditya's program makes them with exactly 5,000 1s and
  -1.  It is also fine if they are based on 10,000 tosses
  of a coin, because once such vectors are multiplied
  together (pointwise), we no longer have exactly 5,000
  anyway.

  If A, B, C, ... denote the vectors for single letters
  "a" "b" "c" ..., then the trigram "abc" is encoded with
  ABC = ssA * sB * C, where s is a circular shift by one
  position, and * is pointwise multiplication (see the
  poster).  Notice that n-grams can be used to encode
  (n+1)-grams.  For example, ABCD = s(ABC) * D.

  Using the Alice text, make separate language vectors
  for the different n-gram sizes (n = 1,2,3, ...,8).  We
  will add them together later into a single language
  vector.  That may involve weighting them unequally--to
  be determined.

  Take the language vector representing single letters of
  Alice and compute its dot product with the 26 different
  letter vectors.  Can you see a relation between the dot
  products and the letter counts in Alice?  Alice has
  about 300 instances of "q".  I'd expect a dot product
  with the Q-vector to be around 3 million.

  Then take the language vector representing the bigrams
  of Alice and compute its dot product with QU.  What do
  you get?  And what is this language vector's dot
  product with Q?

  Next, add the two language vectors into a single vector
  that represents both individual letters and bigrams.
  Compute its dot product with Q and with QU.  What do
  you get?

  One more set of tests: Take the language vector for
  bigrams and (pointwise) multiply it with sQ (Q shifted
  once).  Compare the resulting vector (with dot product
  or cosine) to the letter vectors A, B, C, ....  Which
  letter wins?  Do the same using the language vector for
  individual letters, and once more using a language
  vector that is the sum of the above two.

  At this point, you will be totally confused or
  absolutely delighted.  Please let me know.

  Good luck, best wishes!
   --Pentti
